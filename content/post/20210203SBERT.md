---
title: "Sentence transformerで日本語モデルを学習する使い方 メモ"
description: ""
date: "2021-02-03T15:58:41+09:00"
thumbnail: ""
tags: ["技術系","自然言語処理","BERT","分散表現",技術,sentence transformer]
---
BERTは自然言語処理タスクに強力に応用できるモデルである。

しかし、文章単位の特徴量をうまく取得できない。

\[ CLS\] に文の特徴量が現れるという主張もあるが、
それほどタスクに対して有益な情報は含まれていないと[この論文](https://arxiv.org/abs/1908.10084)は主張する。

文単位の特徴量を取得できるようにBERTを拡張するモデルがSentence BERTである。

以下はSentence BERT を日本語で作成する際の手順になる。

## 環境構築
Google colabでのモデル学習方法を行う。 

```
!pip install -U sentence-transformers
!apt-get install mecab mecab-ipadic-utf8 python-mecab libmecab-dev
!pip install mecab-python3 fugashi ipadic
```
日本語版BERTを使うのでそれに伴ってmecabなどのインストールが必要

## モデル構築
GPUについては指定しない場合、自動で利用する設定になるらしい。
```py
import transformers
from transformers import BertJapaneseTokenizer, BertModel

transformers.BertTokenizer = transformers.BertJapaneseTokenizer

from sentence_transformers import SentenceTransformer
from sentence_transformers import models,InputExample,losses
from sentence_transformers.losses import TripletDistanceMetric, TripletLoss
from sentence_transformers.evaluation import TripletEvaluator
from sentence_transformers.readers import TripletReader
from sentence_transformers.datasets import SentencesDataset
from torch.utils.data import DataLoader

# ver0.4までは以下
#transformer = models.BERT('cl-tohoku/bert-base-japanese-whole-word-masking')
# ver1.以上では以下
transformer = models.Transformer('cl-tohoku/bert-base-japanese-whole-word-masking')

pooling = models.Pooling(
    transformer.get_word_embedding_dimension(), 
    pooling_mode_mean_tokens=True, 
    pooling_mode_cls_token=False, 
    pooling_mode_max_tokens=False
)

model = SentenceTransformer(modules=[transformer, pooling])
```
## データセット作成
センテンスに対してラベルを予測するモデルを考える。

マルチラベルのデータを用意した。
```py
df = pd.read_excel(report_path, dtype={'text':str,'tokens':str,'label':int,"cause":int,"effect":int,})

train_exapmle = []
labels = []
for index1, row1 in df.iterrows():    
    if row1['cause'] == 1:
        label = 1
    elif row1["effect"] == 1:
        label = 2
    else:
        label = 0
        
    train_exapmle.append(InputExample(texts=[row1.tokens], label=float(label)))
    labels.append(label)
```

    InputExample(texts=[row1.tokens], label=float(label))
ここの記述は損失関数の選択によって修正する必要がある。

今回は BatchAllTripletLoss を選択したため、一つの文とラベルのセットになっている。

以下でpytorchでの学習用データローダーを作成する。
```py
train_example = SentenceLabelDataset(train_exapmle, samples_per_label=16)
train_dataloader = DataLoader(train_example, batch_size=32)
```
- batch_sizeは計算環境に応じて調整する。メモリ使用量に影響する。
- samples_per_labelは自動でtriplet datasetを作成する際のサンプリング数らしい。

ここで samples_per_label は batch_size の約数でなければならない。



## 学習

```py
train_loss = losses.BatchAllTripletLoss(model)
model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=30, warmup_steps=100)
```

## 文章の分散表現の獲得
文章のリストを入力すると、numpya.arrayの分散表現が得られる。
```py
nunpy_array = model.encode([str,str,])
```
## モデルの保存
    model.save(path.join(ROOT, "model","sentenceBERT"))

## モデルのロード
    model2 = SentenceTransformer(path.join(ROOT, "model","sentenceBERT"))


## 得られた分散表現の利用方法は以下
[文書分類問題の応用]({{<ref "/post/20200618blog-post_54.md">}})

[分散表現の利用法 バギングによるクラス分類や汎化性能についてのメモ]({{<ref "/post/20210204classifier.md">}})

## 参考リンク
- [公式ドキュメント](https://www.sbert.net/docs/package_reference/losses.html)
- [日本語での使用例](https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part9.html)
